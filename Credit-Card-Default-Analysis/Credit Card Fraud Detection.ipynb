{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('creditcard.csv', index_col=False)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the data has any NAN values. If there are, we need to consider either to impute values or to remove the entire rows. From the output below, there are no NAN values, saving a lot of work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do know that the data is skewed. The graphical output shows the degree of biasness of the data set. The proportion of true frauds is very low and expectedly so. Otherwise the banks will be losing lost of money due to loan defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAETCAYAAADge6tNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118939ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_classes = pd.value_counts(data['Class'], sort = True).sort_index()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Fraud class bar chart\")\n",
    "plt.xlabel(\"Fraud Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline check\n",
    "\n",
    "The bank allows for defaults to happen based on the status quo processes and loan verification, and assume that all the loans have no risk of defaults. What then is the percentage accuracy of default happening based on the data set? \n",
    "\n",
    "The baseline is already 99.83%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.998273\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00    284315\n",
      "          1       0.00      0.00      0.00       492\n",
      "\n",
      "avg / total       1.00      1.00      1.00    284807\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Default accuracy\n",
    "default_predictions=[0 for i in range(len(data))]\n",
    "\n",
    "print(\"Accuracy score %f\" % accuracy_score(data['Class'], default_predictions))\n",
    "print(classification_report(data['Class'], default_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALUAAAC4CAYAAAClza13AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngEIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d9c44e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matrix = confusion_matrix(data['Class'], default_predictions)\n",
    "plot_confusion_matrix(conf_mat=matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some data cleaning effort\n",
    "\n",
    "Although the data set is complete, with all the usable attributes properly scaled, we still need to pre-process the data. 2 things need to be done, (1) scale the values in the 'Amount' column into the range [-1, 1] and (2) drop the Time column. It should be safe to assume that the 'Time' column is not useful for any analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "      <th>Norm_Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10     ...            V22       V23       V24  \\\n",
       "0  0.098698  0.363787  0.090794     ...       0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425 -0.166974     ...      -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  0.207643     ...       0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024 -0.054952     ...       0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  0.753074     ...       0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  Norm_Amount  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0     0.244964  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0    -0.342475  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0     1.160686  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0     0.140534  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0    -0.073403  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.values.reshape used because of deprecation\n",
    "Scaler = StandardScaler().fit(data['Amount'].values.reshape(-1, 1))\n",
    "data['Norm_Amount'] = Scaler.transform(data['Amount'].values.reshape(-1, 1)) \n",
    "data = data.drop(['Time'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over and undersampling of data set\n",
    "\n",
    "To build a learning model, I use all cases of \"Class==1\" from the data set (over sample). The number of such cases 492. I use the same number of samples 492 for \"Class==0\" data set, totalling 590 samples. The 2 data sets are then concatenate to form the 'under_sample_data' data set. This data will then be used to train the base line machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492\n"
     ]
    }
   ],
   "source": [
    "fraud_rows = data[data['Class'] == 1]\n",
    "print(len(fraud_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492\n"
     ]
    }
   ],
   "source": [
    "non_fraud_rows = data[data['Class'] == 0]\n",
    "non_fraud_rows_samples = non_fraud_rows.sample(len(fraud_rows), replace=True, random_state=21)\n",
    "print(len(non_fraud_rows_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_sample_data = pd.concat([non_fraud_rows_samples, fraud_rows])\n",
    "#under_sample_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_undersample = under_sample_data['Class'].values\n",
    "X_undersample = under_sample_data.drop(['Class', 'Amount'], axis=1).values\n",
    "\n",
    "X_undersample_train, X_undersample_test, Y_undersample_train, Y_undersample_test = train_test_split (X_undersample, Y_undersample, test_size = 0.30, random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline algorithm test\n",
    "\n",
    "We run the data set through a set of ML algorithms using the default parameter setting, with 10-Fold validation. From there, we identify the best performing algorithm with the best accuracy values. It looks like the Logistic Regression model is the best performaing algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = []\n",
    "models_list.append(('CART', DecisionTreeClassifier()))\n",
    "models_list.append(('SVM', SVC())) \n",
    "models_list.append(('NB', GaussianNB()))\n",
    "models_list.append(('KNN', KNeighborsClassifier()))\n",
    "models_list.append(('LR', LogisticRegression()))\n",
    "models_list.append(('LDA', LinearDiscriminantAnalysis()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART: 0.888086 (0.022474)\n",
      "SVM: 0.925895 (0.019852)\n",
      "NB: 0.899616 (0.029086)\n",
      "KNN: 0.930158 (0.021632)\n",
      "LR: 0.940409 (0.020943)\n",
      "LDA: 0.920119 (0.030473)\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in models_list:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=1)\n",
    "    cv_results = cross_val_score(model, X_undersample_train, Y_undersample_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print( \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter turning\n",
    "\n",
    "There are a few parameters that require tuning to improve the performance of the Logistic Regression algorithm - 'c_param' and 'penalty'. I use GridSearchCV method to test model thru a series of parameter values. The best settings, according to the outcome below, are C=1 and 'penalty' = l2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875000 (0.033008) with: {'C': 0.01, 'penalty': 'l1'}\n",
      "0.936047 (0.031376) with: {'C': 0.01, 'penalty': 'l2'}\n",
      "0.937500 (0.025232) with: {'C': 0.1, 'penalty': 'l1'}\n",
      "0.937500 (0.019562) with: {'C': 0.1, 'penalty': 'l2'}\n",
      "0.937500 (0.020558) with: {'C': 1, 'penalty': 'l1'}\n",
      "0.940407 (0.020973) with: {'C': 1, 'penalty': 'l2'}\n",
      "0.938953 (0.021375) with: {'C': 10, 'penalty': 'l1'}\n",
      "0.938953 (0.021375) with: {'C': 10, 'penalty': 'l2'}\n",
      "0.930233 (0.030282) with: {'C': 100, 'penalty': 'l1'}\n",
      "0.931686 (0.028315) with: {'C': 100, 'penalty': 'l2'}\n",
      "Best: 0.940407 using {'C': 1, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "c_param = [0.01,0.1,1,10,100]\n",
    "penalty_list = ['l1', 'l2']\n",
    "param_grid = {'C':c_param, 'penalty':penalty_list}\n",
    "model = LogisticRegression()\n",
    "kfold = KFold(n_splits=num_folds, random_state=21)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=kfold)\n",
    "grid_result = grid.fit(X_undersample_train, Y_undersample_train)\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained model on undersampled set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=1, penalty='l2')\n",
    "model.fit(X_undersample_train, Y_undersample_train)\n",
    "\n",
    "predictions = model.predict(X_undersample_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix output below, there are 19 cases of mis-classification out of a total of 296 cases. Accuracy of the trained model is 93.6%. It's much less than the baseline model of 99.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALUAAAC4CAYAAAClza13AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nULvgeKhdcDzULjgeahec/wf23zCOPFxXJAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1164a9470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matrix = confusion_matrix(Y_undersample_test, predictions)\n",
    "plot_confusion_matrix(conf_mat=matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.935811\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.97      0.94       144\n",
      "          1       0.97      0.91      0.94       152\n",
      "\n",
      "avg / total       0.94      0.94      0.94       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score %f\" % accuracy_score(Y_undersample_test, predictions))\n",
    "print(classification_report(Y_undersample_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we test the machine learning model on the entire skewed set, the accuracy increased to 96.4%. The performance improved compared to the run on the undersampled set. However, it's still worse off than the baseline 99.83%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALUAAAC4CAYAAAClza13AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEWhJREFUeJzt3Xl4FFW6x/HvS2QLKkIWCHsGw6pCJAIKo7KDLMommyas\nXlkGF4TAvTOIXoWEcRkXdERkExAdQALIMux6cUEWB4joyKCM0ZgAgQFl13P/6KLTCSRpklS6Pb6f\n58mTqtOnqt7q55fK6erqajHGoJRNSgW6AKWKm4ZaWUdDrayjoVbW0VAr62iolXU01Mo6GmplHQ21\nss5VgS7AV8XrKpnIqtUCXUbQuPbq8oEuIWjs3bP3xLlzZyv60zeoQh1ZtRovzFwc6DKCRodWNwS6\nhKARGRGW6W9fHX4o62iolXU01Mo6GmplHQ21so6GWllHQ62so6FW1tFQK+toqJV1NNTKOhpqZR0N\ntbKOhlpZR0OtrKOhVtbRUCvraKiVdTTUyjoaamUdDbWyjoZaWUdDrayjoVbW0VAr62iolXU01Mo6\nGmplHWtCfTjzByY+NIwH4+9hZEJPUpYsACBpynjGDOvLmGF9GdKvM2OG9c2xXGZGOr07t2Dp4rne\ntnmvv0hCnw707twiR9/1a1IY0OMO7/rWrVrqWccP3zN2+L2MGdaXkQk9WZ3yjrs7Wwgjhg+lelQV\nmja50duWlZVFl04dadSgHl06deTYsWMAHD16lA7t2lKp4jU8NHZMjvWcO3eOkQ8+QKOG9bmhcUOW\nLfM8B395/jluurExN8c2oVOH9hw6dKjkdi4XV+96KiKdgReAEGCWMSbJrW2FhIQwfPQ4rq/XiFOn\nfuKhEf2JjbuViVP+7O0za8YzhFa4Osdys2b8mWbNW+doa3HbHXTvNYARg7pdsp3b23Zi5MP/naOt\nUlgEz76ygNJlynD61ClGDelFi1Z3EhYeWYx7WDTx8YMZNWoMQ4YkeNumJyfRpm1bJiROZHpyEtOT\nk5iWlEy5cuWY8sSTpKbuIzV1X471TJv6NBERkXy+/0t++eUXsrKyAGjaNJaPP/mU0NBQXvvrq0ya\nmMiitwJzB1vXjtQiEgLMALoAjYABItLIre1VDovg+nqe1YeGVqBm7WiOHs6++6sxhg82r+OO9l28\nbR99sIkqUdWpHV03x7oaNG5C5bAIv7ddunRpSpcpA8D58+cwv/xSlF1xxe9vv51KlSvnaFu5cgX3\nx3tCfn98AitWpABQoUIFWrVuTbly5S5Zz7y5c0icOAmAUqVKER4eDsCdbdoQGhoKQPMWLfkuLc21\nfSmIm8OP5sABY8xBY8w5YDFwt4vb88pI/46DX31B/UbZ/2pT9+zkusphVK9RG4DTp06xZNFsBiaM\nvKJ1b9u6gVGDezF18qMczvzB23448wdGD+nN4L4d6TNwaFAdpfOSmZFBVFQUAFWrViUzIyPf/seP\nHwdgyuQ/0fyWZvTvdy8Zl1lm7pw36NS5c/EX7Cc3Q10d+NZnPs1py0FEHhCRHSKy4z/HjxV5o6dP\nneLpyY8y4g8Tcgw1tm5Ywx3tso/SC+e+wj1976e8c3TxR4vb7mDO22t5Ze4yYuNu5bmp/+N9LCKy\nKjPmLOX1RavYuHYFx7KOFnlfSpKIICL59rlw4QJpaWm0vO02tn+6k5YtW5I4YXyOPgsXLmDnjp2M\ne2x8HmtxX8BfKBpjZhpj4owxcRWvq1SkdV24cJ6pkx+lTfuutLq9vbf95wsX+PCDjdzeppO37Z+f\n72X2a88zpF9nUpYs5J0Fs1i57K18139txeu8w4yOXXtx4J/7L+kTFh5J7ejrSd2zs0j7UhIiq1Qh\nPT0dgPT0dCIi8//vEhYWRmhoKD179gKgd5++7N69y/v4xg0bSJo2lWXLUyhbtqx7hRfAzVB/B9T0\nma/htLnCGMMLyY9Ts3Y0PfvF53hs986PqVErmvDIqt626S/PY87ba5nz9lru7jOIe+8bTvdeA/Ld\nRtbRw97pT7ZtoWbtaACOZP7A2bNnADh58gSpe3dTo2adYtoz93Tv1p03588D4M358+jevUe+/UWE\nrt26s3XLFgA2b9pIw4ae1zG7d+9m9KgHWfZuCpEF/HG4zc2zH58CMSISjSfM/YGBbm3s87272fT3\nVdT5XYz3tF3CiLHc0vL3vL9pbY6hR0Fmv/ocWzau5uyZM8T3aU+nrr0YNGQUK5Yu4pNtWwgJCeHq\nayryyMSnAPj20NfMeuUZRARjDL36JVCnbj1X9rOw7hs0kPe3buHIkSNE167J5MenMD5xIgP792Pu\nnNnUqlWbRYvf9vaPqRvNiRMnOHfuHCtSUnhvzToaNWrE1GlJDEmIZ9y4R4gIj+D1N2YDMClxAj/+\n+CMD+t8LQM2atXh3eUpA9lWMMe6tXOQu4C94TunNNsY8nV//mAaNjX6RUTb9IqNskRFhB45lZcX4\n09fV89TGmNXAaje3oVRuAX+hqFRx01Ar62iolXU01Mo6GmplHQ21so6GWlknz/PUInISuPjOzMUr\nXYwzbYwx17pcm1KFkmeojTHXlGQhShUXv4YfItJaRIY40+HO9RxKBaUCQy0ijwOJwCSnqQywwM2i\nlCoKf47UPYEewE8AxpjvAR2aqKDlT6jPGc+lfAZARCq4W5JSReNPqN8RkdeA60RkBLABeN3dspQq\nvAIvPTXGPCMiHYATQD1gsjFmveuVKVVI/l5PvRcoj2cIste9cpQqOn/OfgwHtgO9gD7AxyIy1O3C\nlCosf47U44FYY8xRABEJAz4EZrtZmFKF5c8LxaPASZ/5k06bUkEpv2s/HnUmDwCfiEgKnjH13cCe\nEqhNqULJb/hx8Q2Wfzk/FwXmc+9K+Sm/C5qeKMlClCouBb5QFJEIYALQGPDeBtMY09bFupQqNH9e\nKC4EvgCigSeAb/DcfUmpoORPqMOMMW8A540xW40xQwE9Squg5c956vPO73QR6Qp8D1TOp79SAeVP\nqJ8SkYrAOOAl4FrgEVerUqoI/LmgaZUz+R+gjbvlKFV0+b358hLZH7y9hDFmbHEXU/Hq8nRqfWPB\nHdVvTv7fcZBTfkfqHUUtRKlAyO/Nl3klWYhSxUVvZqOso6FW1tFQK+v488mXeiKyUUT2OfM3icgf\n3S9NqcLx50j9Op4b2ZwHMMbswfNNW0oFJX9CHWqM2Z6r7YIbxShVHPwJ9RERqUv2zWz6AOmuVqVU\nEfhz7cdoYCbQQES+A74G7nO1KqWKwJ9rPw4C7Z3bjZUyxpwsaBmlAsmfT75MzjUPgDHmSZdqUqpI\n/Bl+/OQzXQ7oBux3pxylis6f4cezvvMi8gywzrWKlCqiwryjGArUKO5ClCou/oyp95J9XXUIEAHo\neFoFLX/G1N18pi8AGcYYffNFBa18Qy0iIcA6Y0yDEqpHqSLLd0xtjPkZ+FJEapVQPUoVmT/Dj0pA\nqohsx+f0njGmh2tVKVUE/oT6T65XoVQx8ifUdxljEn0bRCQZ2OpOSUoVjT/nqTtcpq1LcReiVHHJ\n774fI4FRwO9ExPcm69cA29wuTKnCym/4sQhYA0wDJvq0nzTGZLlalVJFkN99P/6D51ZjA0quHKWK\nTj9NrqyjoVbW0VAr62iogZ9//pm4ZrH06O65disrK4tOHTvQoH4MnTp24NixYwGu0F259/+JJ6ZQ\nq2Z1mt3clGY3N2X16tUArF+/nua3NKNpkxtpfkszNm3aFMiy8+RaqEVktohkXrwJTjB78cUXaNCg\noXc+OTmJtu3a8cWXX9G2XTuSk5MCWJ37cu8/wEMPP8LOXZ+xc9dn3HXXXQCEh4ezPGUln/1jL7Pn\nzGNwwv2BKLdAbh6p5wKdXVx/sUhLS2P16vcYOmy4t23lihTi4xMAiI9PYEXK8kCV57rL7X9eYmNj\nqVatGgCNGzfm9OnTnD171u0Sr5hroTbGvA8E/fnsRx95mKSk6ZQqlf1UZGRkEBUVBUDVqlXJyMgI\nVHmuu9z+A8x4+SVim97E8GFDLzv8WrZ0KbE330zZsmVLqlS/BXxMLSIPiMgOEdlx+PDhEt32qlWr\niIyMpFmzZnn2ERHvJ+htk9f+P/jgSL46cJCduz6jalQU4x8bl+Px1NRUJk1K5NVXXyvJcv3mzwVN\nrjLGzMRzsxzi4uLy/DoON3z44TZWrlzBmjWrOXPmDCdOnCD+/vuoUqUK6enpREVFkZ6eTmRkZEmW\nVWLy2v/5by7w9hk+fAR398j+8FNaWhp9evdkztz51K1bNxBlFyjgR+pAmjp1Gof+nca/Dn7DwkWL\nadOmLfPfXEC37j2YP9/zRQrz58+je4+7A1ypO/La//T07LvKLV/+Lo0b3wDA8ePH6dG9K1OnJtGq\nVatAlV2g33So85KYOJENG9bToH4MGzduIDFxYsELWWRi4gSaNrmR2KY3sWXzZp597nkAZsx4mQMH\nDvDUU096T/dlZmYGuNpLiTHu/McXkbeAO4FwIAN43Pnm3DzFxcWZT7br9yepS0WEVz6QlZUV409f\n18bUxhi9EEoFhA4/lHU01Mo6GmplHQ21so6GWllHQ62so6FW1tFQK+toqJV1NNTKOhpqZR0NtbKO\nhlpZR0OtrKOhVtbRUCvraKiVdTTUyjoaamUdDbWyjoZaWUdDrayjoVbW0VAr62iolXU01Mo6Gmpl\nHQ21so5rdz0tDBE5DBwKdB147tR6JNBFBJFgeD5qG2Mi/OkYVKEOFiKywxgTF+g6gsWv7fnQ4Yey\njoZaWUdDfXkzA11AkPlVPR86plbW0SO1so6GWllHQ+1DRDqLyJcickBEflvfM5eLiMwWkUwR2Rfo\nWq6UhtohIiHADKAL0AgYICKNAltVQM0FOge6iMLQUGdrDhwwxhw0xpwDFgN2ftWtH4wx7wNZga6j\nMDTU2aoD3/rMpzlt6ldGQ62so6HO9h1Q02e+htOmfmU01Nk+BWJEJFpEygD9gRUBrkkVgobaYYy5\nAIwB1gH7gXeMMamBrSpwROQt4COgvoikiciwQNfkL32bXFlHj9TKOhpqZR0NtbKOhlpZR0OtrKOh\ndomI/Oj8riYiSwro+7CIhF7h+u8UkVX+tufqM1hEXr7C7X0jIuFXskygaKivgHMl3xUxxnxvjOlT\nQLeHgSsKtcqbhhoQkToi8oWILBSR/SKy5OKR0zlCJYvILqCviNQVkbUislNEPhCRBk6/aBH5SET2\nishTuda9z5kOEZFnRGSfiOwRkT+IyFigGrBZRDY7/To669olIn8Tkaud9s5OnbuAXn7sV3NnPbtF\n5EMRqe/zcE0R2SIiX4nI4z7L3Cci20XkMxF5rTB/yAFnjPnN/wB1AAO0cuZnA485098AE3z6bgRi\nnOkWwCZnegUQ70yPBn70Wfc+Z3oksAS4ypmv7LONcGc6HHgfqODMJwKTgXJ4riKMAQR4B1h1mX25\n82I7cK3PttoDS53pwUA6EAaUB/YBcUBDYCVQ2un3is8+eWsM9p+rCvF3YKtvjTHbnOkFwFjgGWf+\nbQDniHkb8DcRubhcWed3K6C3M/0mkHyZbbQH/mo8b8ljjLnc9cot8XxIYZuzjTJ43q5uAHxtjPnK\nqWUB8EAB+1QRmCciMXj+aEv7PLbeGHPUWdcyoDVwAWgGfOpsuzyQWcA2go6GOlvu6wV8539yfpcC\njhtjmvq5jsIQPIEbkKNRJK9t5ud/gc3GmJ4iUgfY4vPY5fZXgHnGmEmF2FbQ0DF1tloicqszPRD4\nv9wdjDEngK9FpC+AeDRxHt6G58o+gEF5bGM98F8icpWzfGWn/SRwjTP9MdBKRK53+lQQkXrAF0Ad\nEanr9MsR+jxUJPvy2cG5HusgIpVFpDxwj1P/RqCPiERerE9EavuxnaCioc72JTBaRPYDlYBX8+g3\nCBgmIv8AUsn+yNdDzvJ7yfsTM7OAfwN7nOUHOu0zgbUistkYcxhPAN8SkT04Qw9jzBk8w433nBeK\n/gwLpgPTRGQ3l/5X3g4sBfbgGWvvMMZ8DvwR+Luz7fVAlB/bCSp6lR6eMxR4XlzdEOBSVDHQI7Wy\njh6plXX0SK2so6FW1tFQK+toqJV1NNTKOv8PN0NCTAp3MwQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a0c6f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y = data['Class'].values\n",
    "X = data.drop(['Class', 'Amount'], axis=1).values\n",
    "\n",
    "predictions = model.predict(X)\n",
    "\n",
    "matrix = confusion_matrix(Y, predictions)\n",
    "plot_confusion_matrix(conf_mat=matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.964179\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98    284315\n",
      "          1       0.04      0.92      0.08       492\n",
      "\n",
      "avg / total       1.00      0.96      0.98    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score %f\" % accuracy_score(Y, predictions))\n",
    "print(classification_report(Y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(Y, predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it worth doing the model to predict default risk?\n",
    "\n",
    "Assuming nothing was done, there will be 492 cases of default based on the given data set. Based on the trained machine learning model, we can pick up 452 defaults out of the 492 actual cases. Intuitively, it seems to be worthwhile to use this model to help reduce the default rate the bank is experiencing currently. \n",
    "\n",
    "However, we have to also consider the number detected fraud increasing from 0 to 10162. From bank's operation's point of view, there are now over 10 thousand cases of suspected fraud for the officer to review. This creates additional work and associated cost (e.g., slower approvals, unhappy customers, more manpower needed for checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(data['Class'], default_predictions)\n",
    "plot_confusion_matrix(conf_mat=matrix)\n",
    "plt.show()\n",
    "\n",
    "matrix = confusion_matrix(Y, predictions)\n",
    "plot_confusion_matrix(conf_mat=matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the performance of algorithm by dimensions reduction\n",
    "\n",
    "Noise within the dataset will affect the prediction outcome. We now try to reduce the data dimension by using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_undersample_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the top 5 components with the highest variance\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(X_undersample_train)\n",
    "X_pca_undersample_train = pca.transform(X_undersample_train)\n",
    "X_pca_undersample_test = pca.transform(X_undersample_test)\n",
    "X_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we test the revised algorithm on the undersampled set, the performance does not change much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=1, penalty='l2')\n",
    "model.fit(X_pca_undersample_train, Y_undersample_train)\n",
    "\n",
    "predictions = model.predict(X_pca_undersample_test)\n",
    "matrix = confusion_matrix(Y_undersample_test, predictions)\n",
    "plot_confusion_matrix(conf_mat=matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we used the revised model on the entire data set, the number of false detection reduced by 50% while the number of missed detection increased by 15. Intuitively, this result is better than earlier because of the manpower effort savings from reviewing the suspected default cases. The overall accuracy of the prediction model is 98.0% (although this is still lower than the baseline accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_pca)\n",
    "\n",
    "matrix = confusion_matrix(Y, predictions)\n",
    "plot_confusion_matrix(conf_mat=matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score %f\" % accuracy_score(Y, predictions))\n",
    "print(classification_report(Y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the tangible benefit to the bank by using this model?\n",
    "\n",
    "To assess if deploying a model such as this provides any tangible benefits to the bank, we need to compare how much defaults the bank could have prevented versus the cost of effort needed to prevent the defaults. \n",
    "\n",
    "In mathematical term, we want to check for \\$(default prevented) >> \\$(effort to check for default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_table = pd.DataFrame()\n",
    "comparison_table['Amount'] = data['Amount']\n",
    "comparison_table['Original_class'] = data['Class']\n",
    "comparison_table['Predicted_class'] = predictions\n",
    "comparison_table.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected = comparison_table['Amount'][(comparison_table['Original_class'] == 1) & (comparison_table['Predicted_class'] == 1)]\n",
    "print(\"Total Default Amount Detected $%0.2f\" % sum(detected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the verification of each predicted fraud case cost is $200, and we decide to verify all the predicted frauds, what is the total verification cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = comparison_table['Predicted_class'].value_counts()\n",
    "print(\"Total predicted default cases: %d\" % (predicted[1]))\n",
    "print(\"Total Verification Cost at $200 each: $%0.2f\" % (predicted[1] * 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data shows that the default rate for this lender is low. The default percentage is only 0.17%. In another words, 99.83% of the debtors returned what they have borrowed. For any machine algorithm to be useful, it must be able to accurately detect these 0.17% (RECALL rate) defaults without too many false detection (Precision). Based on the AUC, performance of the machine learning tested so far is not ideal for deployment. The overall cost associated with the verification of the potential defaults is much higher than the actual defaults. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
